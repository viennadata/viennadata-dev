\chapter{Benchmark Results} \label{chap:benchmarks}

\NOTE{This documentation hasn't been updated to the interface of the upcoming ViennaData 1.1.0 release yet}


Since {\ViennaData} introduces a layer for handling associated data, it is rather
natural to ask for any potential runtime overhead. The aim of this
chapter is to give a precise answer. Tests have been carried out on a standard desktop computer running Ubuntu 10.10.

\section{ViennaData Storage Models}
There are the following three different object-to-data models available in {\ViennaData}:
\begin{itemize}
 \item Object identification via object addresses, sparse storage.
 \item Custom identification via dedicated \lstinline|id()| member, sparse storage.
 \item Custom identification via dedicated \lstinline|id()| member, dense storage (cf.~Section \ref{sec:dense-data}).
\end{itemize}
In addition, the following data key dispatch mechanisms are compared in the following:
\begin{itemize}
 \item Key dispatch at run time for keys of type \lstinline|std::string|.
 \item Key dispatch at run time for keys of type \lstinline|long|.
 \item Key dispatch at compile time (cf.~Section \ref{sec:compiletime-keys}).
\end{itemize}
This results in nine different configurations for the subsequent tests. All comparisons are carried out for objects of the following class:
\begin{lstlisting}
class SlimClass
{
  public:
    SlimClass(double v = 1.0, size_t i = 0) : value_(v), id_(i) {}

    double value() const { return value_; }
    size_t id() const { return id_; }
  private:
    double value_;
    size_t id_;
};
\end{lstlisting}

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|ccc|}
\hline
 & \multicolumn{3}{c|}{ Key Dispatch } \\
\hline
Identification       & \lstinline|std::string| & \lstinline|long| & compile time\\
\hline
\lstinline|&obj, sparse|     & 84 & 53 & 43 \\
\lstinline|obj.id(), sparse| & 83 & 52 & 43 \\
\lstinline|obj.id(), dense|  & 48 & 15 & \textbf{4}\\
\hline
\end{tabular}
\end{center}

\caption{Execution time for summing up data of $1\, 000$ objects of type \lstinline|SlimClass|, repeated $1\, 000$ times. Execution times in milliseconds.}
\label{tab:benchmark-different-schemes}
\end{table}

The first test with execution times given in Table \ref{tab:benchmark-different-schemes} compares the execution times of summing up random values
stored and retrieved for $1\, 000$ objects of type \lstinline|SlimClass| using \lstinline|viennadata::access<>()|, repeated $1\,000$ times.

Let us first consider the differences among the three rows. As can clearly be seen, a change of the object identification from address-based to using the member function \lstinline|id()| does not change execution speed,
if the internal storage scheme remains tree-based (\emph{sparse}).
However, if the \lstinline|id()| member is used for a dense storage scheme as described in Section \ref{sec:dense-data}, execution times are reduced by around $40$ milliseconds
irrespective of the key dispatch method employed.

Comparing the different columns in Table \ref{tab:benchmark-different-schemes}, it can be seen that a string-based dispatch is most expensive as expected.
A dispatch based on integers is faster, but the differences only become significant, if a dense storage scheme is used. The compile-time dispatch is notably faster than dispatches at run time.
In particular, execution times differ by a factor of four and twelve for the dense storage scheme respectively.



\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|ccc|}
\hline
 & \multicolumn{3}{c|}{ Key Dispatch } \\
\hline
Identification       & \lstinline|std::string| & \lstinline|long| & compile time\\
\hline
\lstinline|&obj, sparse|     & 333 & 247 & 173 \\
\lstinline|obj.id(), sparse| & 318 & 228 & 178 \\
\lstinline|obj.id(), dense|  & 151 & 73 & \textbf{5}\\
\hline
\end{tabular}
\end{center}

\caption{Execution time for summing up data of $1\: 000 \: 000$ objects of type \lstinline|SlimClass|. Execution times in milliseconds.}
\label{tab:benchmark-different-schemes-2}
\end{table}

In a second benchmark, the same procedure is carried out for $1\: 000 \: 000$ objects without repetition. Thus, a naive guess is that execution times should be roughly the same.
However, as Table \ref{tab:benchmark-different-schemes-2} shows, the logarithmic lookup times in a sparse, tree-based storage layout leads to much higher execution times than for the first test.
This may be also partly due to less efficient CPU cache use caused by the larger amount of data involved.

Finally, it should be emphasized that in real applications there is typically much less emphasis on data retrieval as compared to our benchmarks. Thus, even though a sparse storage scheme
is slower than a dense storage scheme, it may be fast enough for most applications.


\section{ViennaData vs. Traditional OOP}
As discussed in the introduction already, {\ViennaData} allows to design small, lightweight classes with high reusability. In this section we compare data access times of {\ViennaData} with
other classes that stem from traditional OOP design and thus have a higher number of class member variables.

The objects for which data is attached using {\ViennaData} are of type \lstinline|SlimClass| as introduced in the previous section. The following three access models are used:
\begin{enumerate}
 \item Object identification via object addresses, sparse storage, key dispatch at run time for keys of type \lstinline|std::string|.
 \item Custom identification via dedicated \lstinline|id()| member, sparse storage, key dispatch at run time for keys of type \lstinline|long|.
 \item Custom identification via dedicated \lstinline|id()| member, dense storage, key dispatch at compile time.
\end{enumerate}
Note that the three models represents the diagonals in Table \ref{tab:benchmark-different-schemes} and Table \ref{tab:benchmark-different-schemes-2}.

For the traditional OOP classes, we use the following template class
\begin{lstlisting}
template <size_t num_bytes>
class FatClass
{
  public:
    FatClass(double v = 1.0, size_t i = 0) : value_(v), id_(i) {}

    double value() const { return value_; }
    size_t id() const { return id_; }
  private:
    double value_;
    char payload[num_bytes];
    size_t id_;
};
\end{lstlisting}
where the template parameter \lstinline|num_bytes| denotes the number of bytes used for other data members.
\lstinline|FatClass<10>|, \lstinline|FatClass<100>| and \lstinline|FatClass<1000>| are used.


\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Setup       & Exec.~Time (us), $10^3$ Objects & Exec.~Time (ms), $10^6$ Objects\\
\hline
{\ViennaData}, config 1 & 84 & 333 \\
{\ViennaData}, config 2 & 52 & 228 \\
{\ViennaData}, config 3 & \textbf{4} & \textbf{5} \\
OOP, \lstinline|FatClass<10>|   & \textbf{1.3} & \textbf{4} \\
OOP, \lstinline|FatClass<100>|  & 2.1 & 11 \\
OOP, \lstinline|FatClass<1000>| & 2.5 & 11 \\
\hline
\end{tabular}
\end{center}
\caption{Execution time for summing up one \lstinline|double| value from each object.}
\label{tab:benchmark-ViennaData-vs-OOP}
\end{table}

As can be seen in Table \ref{tab:benchmark-ViennaData-vs-OOP}, access times with dense storage are by a factor of two larger for $1\,000$ objects, but by a factor of two smaller when using $1\,000\,000$ objects.
Thus, {\ViennaData} allows to achieve even better execution performance than traditional OOP for a large number of objects. The reason is that the relevant data is located in a contiguous piece of memory when using {\ViennaData} with a dense data layout, thus offering better caching possibilities. On the other hand, with standard OOP the memory layout is such that relevant data for each object is followed by payload data and this pattern is repeated for each object. Thus, relevant data is scattered over a larger memory region, reducing caching possibilities.

The sparse storage layout is not competitive in terms of access times as the number of objects grows. However, data access times are often negligible compared to data manipulation, thus
the increased data access time may be in many cases still small on the overall.
